---
title: "Structural Safety"
author: "Lily Jiang"
date: 2023-03-27
output:
  github_document:
    toc: true
prerequisites:
  - e-vis09-bootstrap
---

*Purpose*: Most real problems have multiple sources of uncertainty mixed together. Untangling these different sources can be challenging, even on a conceptual level. In this challenge we'll study a simple problem of structural safety, and use this small case study to untangle sources of sampling and Monte Carlo uncertainty.

*Note*: In this challenge I provide a lot of stub-code; you won't have to do too much coding. This is because I want you to *focus on answering the conceptual questions*. The most important questions in this challenge are: *What sources of uncertainty are you accounting for? What sources are you not accounting for? Are those sources real or induced?*

<!-- include-rubric -->

# Grading Rubric

<!-- -------------------------------------------------- -->

Unlike exercises, **challenges will be graded**. The following rubrics define how you will be graded, both on an individual and team basis.

## Individual

<!-- ------------------------- -->

| Category    | Needs Improvement                                                                                                | Satisfactory                                                                                                               |
|-------------------|----------------------------|-------------------------|
| Effort      | Some task **q**'s left unattempted                                                                               | All task **q**'s attempted                                                                                                 |
| Observed    | Did not document observations, or observations incorrect                                                         | Documented correct observations based on analysis                                                                          |
| Supported   | Some observations not clearly supported by analysis                                                              | All observations clearly supported by analysis (table, graph, etc.)                                                        |
| Assessed    | Observations include claims not supported by the data, or reflect a level of certainty not warranted by the data | Observations are appropriately qualified by the quality & relevance of the data and (in)conclusiveness of the support      |
| Specified   | Uses the phrase "more data are necessary" without clarification                                                  | Any statement that "more data are necessary" specifies which *specific* data are needed to answer what *specific* question |
| Code Styled | Violations of the [style guide](https://style.tidyverse.org/) hinder readability                                 | Code sufficiently close to the [style guide](https://style.tidyverse.org/)                                                 |

## Due Date

<!-- ------------------------- -->

All the deliverables stated in the rubrics above are due **at midnight** before the day of the class discussion of the challenge. See the [Syllabus](https://docs.google.com/document/d/1qeP6DUS8Djq_A0HMllMqsSqX3a9dbcx1/edit?usp=sharing&ouid=110386251748498665069&rtpof=true&sd=true) for more information.

# Setup

<!-- ----------------------------------------------------------------------- -->

```{r setup}
library(MASS)
library(rsample)
library(broom)
library(tidyverse)
filename_samples <- "./data/al_samples.csv"

```

*Background*: The [strength](https://en.wikipedia.org/wiki/Ultimate_tensile_strength) of a material is the amount of mechanical stress it can survive before breaking. To illustrate: Stresses are internal forces that hold an object together when we try to squeeze, stretch, or otherwise deform a solid object. For instance, if we pull on a rectangular bar of material, internal stresses $\sigma$ work to keep the bar together.

![Stress](./images/stress.png)

By Jorge Stolfi - Own work, CC BY-SA 3.0, <https://commons.wikimedia.org/w/index.php?curid=24499456>

*Strength* is a measure of how much internal stress an object can survive: A higher strength means a stronger material. This challenge's data are very simple: They are (synthetic) observations of ultimate tensile strength (UTS) on individual samples of the same aluminum alloy. The experiments were carried out following the highest standards of experimental rigor, so you should consider these values to be the "true" breaking strength for each sample.

```{r read-data}
## NOTE: No need to edit; load data
df_samples <- read_csv(filename_samples)
df_samples
```

Data Dictionary:

| Quantity       | Units |
|----------------|-------|
| Strength (UTS) | psi   |

### **q1** Visualize the strength data with a histogram. Answer the questions below.

```{r q1-task}
df_samples %>%
  ggplot(mapping = aes(x = strength)) +
  geom_histogram(bins = 30)
```

**Observations**:

-   What is the mean strength of the material, approximately?
    -   It appears that the mean strength is around 40,000 psi.
-   To what extent can you tell what shape the distribution of the data has?
    -   It is very hard to tell what the true shape of the distribution is. It could be normally distributed, and it could honestly also be skewed right. It might even be bimodal, considering the leftmost bar of the graph.
-   Assuming the scopus is the strength of an individual part made from this aluminum alloy, is the observed variability real or induced?
    -   Both, but also arguably just real. The real variability comes from the inherent variation between each alloy piece tested. The induced variability comes from the actual tests, since it isn't possible to have a perfect measurement system. But because it was specified that the dataset values should be considered the "true" breaking strengths, it could also be argued that there is no induced variability (or even just negligible induced variability).

# Assessing Structural Safety

<!-- ----------------------------------------------------------------------- -->

*Objective*: In this challenge you are going to study a structure and assess its *probability of failure* (POF). A higher POF corresponds to a more unsafe structure. Ultimately, we want

$$\text{POF} < 0.03.$$

Your job is to assess a given structure using the data provided and determine whether you can *confidently* conclude that `POF < 0.03`.

## Structural model

<!-- --------------------------------------- -->

The following code chunk sets up a structural model: We are considering a rectangular bar under uniaxial tensile load (as pictured above). A larger cross-sectional area `A` is capable of surviving a greater load `L`, but a larger `A` is a heavier (more expensive) structure. The internal stress is approximately `sigma = L / A`---we simply compare this quantity against the strength.

I pick particular values for `A, L` and package all the information in the *limit state function* `g_break`:

```{r model-setup}
## NOTE: No need to edit; model setup
A <- 0.0255 # Cross-sectional area
L <- 1000 # Applied load (lbs)

g_break <- function(strength) {
  strength - L / A
}
```

The *probability of failure* (POF) is then defined in terms of the limit state $g$ via:

$$\text{POF} \equiv \mathbb{P}[g \leq 0].$$

### **q2** Using the observations in `df_samples` and the structural model `g_break()`, estimate the probability of failure.

*Hint*: In `c07-monte-carlo` you learned how to estimate a probability as the `mean()` of an indicator. Use the same strategy here.

```{r q2-task}
df_samples %>%
  mutate(
    stat = ifelse(g_break(strength) <= 0, 1, 0)
  ) %>%
  summarise(pof = mean(stat))
```

**Observations**:

-   Does this estimate satisfy `POF < 0.03`?
    -   Yes. The POF is perfectly 0 within this sample, indicating that all the tested alloy pieces have a greater strength than stress.
-   Is this estimate of the probability of failure trustworthy? Why or why not?
    -   Not really. This sample size is only 25, which is actually a rather small sample to be representative of all aluminum alloys on its own. Perhaps if many samples of size 25 were used then the trustworthiness might be different. Also, we don't really have information about how randomly the alloys in this sample were chosen, so it is possible that there is bias influencing the outcome.
-   Can you confidently conclude that `POF < 0.03`? Why or why not.
    -   No. Again, the sample size is quite small to be confidently making conclusions, and more context for the randomness of a sample would be useful.

## Material property model

<!-- --------------------------------------- -->

Since we have so few physical samples, we will fit a distribution to model the material property. This will give us the means to draw "virtual samples" and use those to estimate the POF.

### **q3** Fit a lognormal distribution to the strength data using the `fitdistr()` function.

*Note*: In this challenge I generated the `strength` data from a `lognormal` distribution; if you didn't know that fact, then the choice of distribution would be an *additional* source of uncertainty!

*Hint 1*: We learned how to do this in `e-stat08-fit-dist`.

*Hint 2*: The `fitdistr` function uses `densfun = "lognormal"` to specify a lognormal distribution.

```{r q3-task}
df_fit <- df_samples %>%
  pull(strength) %>%
  fitdistr(densfun = "lognormal") %>%
  tidy()
df_fit
```

Once you've successfully fit a model for the strength, you can estimate the probability of failure by drawing samples from the fitted distribution.

### **q4** Complete the code below by 1. choosing a Monte Carlo sample size `n_monte_carlo`, 2. extracting the estimated parameters from q3, and 3. computing the limit state value `g = g_break()`. Answer the questions under *observations* below.

*Hint 1*: You will need to combine ideas from `c07-monte-carlo` and `e-stat08-fit-dist` in order to complete this task.

*Hint 2*: The function `rlnorm()` will allow you to draw samples from a lognormal distribution.

```{r q4-task}
## Choose Monte Carlo sample size
n_monte_carlo <- 1000

## 2: Extract parameter estimates from df_fit
strength_meanlog <- df_fit %>%
  filter(term == "meanlog") %>%
  pull(estimate)
strength_sdlog <- df_fit %>%
  filter(term == "sdlog") %>%
  pull(estimate)

set.seed(1)

# Generate samples
df_norm_sim <-
  tibble(strength = rlnorm(n_monte_carlo, strength_meanlog, strength_sdlog)) %>%
  mutate(
    g = g_break(strength),
    breakable = (g <= 0)
  ) %>%
  glimpse()

## NOTE: The following code estimates the POF and a 95% confidence interval
df_norm_pof <-
  df_norm_sim %>%
  mutate(stat = g <= 0) %>%
  summarize(
    pof_est = mean(stat),
    se = sd(stat) / sqrt(n_monte_carlo)
  ) %>%
  mutate(
    pof_lo = pof_est - 1.96 * se,
    pof_hi = pof_est + 1.96 * se
  ) %>%
  select(pof_lo, pof_est, pof_hi)

df_norm_pof
```

-   Assuming your scopus is the probability of failure `POF` defined above, does your estimate exhibit real variability, induced variability, or both?
    -   Only induced variability. The POF estimate depends on the distribution of strength values, and the distribution isn't changing. Therefore, the POF is essentially a constant. Also because the distribution is unchanging, there is no real variability here.
-   Does this confidence interval imply that `POF < 0.03`?
    -   No. The confidence interval of 0.028 - 0.038 includes 0.03 within its range. The true POF value could fall *anywhere* within this range, which does not guarantee that the POF will be less than 0.03.
-   Compare this probability with your estimate from q2; is it more or less trustworthy?
    -   It is more trustworthy, because this probability involves estimates and ranges that recognize the uncertainty involved.
-   Does the confidence interval above account for uncertainty arising from the *Monte Carlo approximation*? Why or why not?
    -   Yes. Confidence intervals are commonly used to account for uncertainty, as they provide a range of possible values instead of a singular, concrete value.
-   Does the confidence interval above account for uncertainty arising from *limited physical tests* (`df_samples`)? Why or why not?
    -   No, because the calculations for the confidence interval are just using a general formula applicable to many datasets. Not all datasets have the same kind or amount of uncertainty, so it is impossible for this general confidence interval formula to account for each kind of uncertainty.
-   What could you do to tighten up the confidence interval?
    -   Having a larger dataset will shrink the confidence interval.
-   Can you *confidently* conclude that `POF < 0.03`? Why or why not?
    -   No. My confidence interval already includes 0.03 within it, which means it is still possible that the POF can be *greater* than 0.03. That already prevents me from being able to make that conclusion. But even if my confidence interval's upper bound was below 0.03, I still wouldn't confidently conclude that POF \< 0.03 based on this specific sample because I still see a lot of uncertainty and variability.

## A different way to compute the POF

<!-- --------------------------------------- -->

Monte Carlo is a *general* way to estimate probabilities, but it introduces approximation error. It turns out that, for the simple problem we're studying, we can compute the probability directly using the CDF. Note that for our structural safety problem, we have

$$\text{POF} = \mathbb{P}[g \leq 0] = \mathbb{P}[S \leq L / A] = \text{CDF}_S(L/A).$$

Since `S = rlnorm(n, meanlog = strength_meanlog, sdlog = strength_sdlog)`, we can use `plnorm` to compute the probability of failure without Monte Carlo as `POF = plnorm(L/A, meanlog = strength_meanlog, sdlog = strength_sdlog)`. Let's combine this idea with the fitted distribution to estimate the POF.

### **q5** Finish the following function by computing the POF with `plnorm()`. Answer the questions under *observations* below.

```{r q5-task}
estimate_pof <- function(df) {
  ## Fit the distribution
  df_fit <-
    df %>%
    pull(strength) %>%
    fitdistr(densfun = "lognormal") %>%
    tidy()

  ## Extract the parameters
  strength_meanlog <-
    df_fit %>%
    filter(term == "meanlog") %>%
    pull(estimate)
  strength_sdlog <-
    df_fit %>%
    filter(term == "sdlog") %>%
    pull(estimate)

## Estimate the probability of failure using plnorm
  pof_estimate <- (L / A) %>%
    plnorm(
      meanlog = strength_meanlog,
      sdlog = strength_sdlog
    )

  ## NOTE: No need to edit; this last line returns your pof_estimate
  pof_estimate
}

## NOTE: No need to edit; test your function
df_samples %>% estimate_pof()
```

**Observations**:

-   How does this estimate compare with your Monte Carlo estimate above?
    -   Monte Carlo gave me an estimate of 0.028, which is larger than this estimate. However, this estimate is still within the confidence interval produced by the Monte Carlo method.
-   Does this estimate have any uncertainty due to *Monte Carlo approximation*? Why or why not?
    -   No. This estimate doesn't use the Monte Carlo method so it wouldn't have any of the uncertainties that come with the method.
-   With the scopus as the `POF`, would uncertainty due to *Monte Carlo approximation* be induced or real?
    -   Induced. The Monte Carlo method simply introduces another way of measuring data, so the uncertainty is induced. To introduce real uncertainty, the method would need to change the actual dataset itself.
-   Does this estimate have any uncertainty due to *limited physical tests*? Why or why not?
    -   Yes. This estimate still relies on the same dataset as before, which we already concluded had uncertainty because of limited physical tests.
-   With the scopus as the `POF`, would uncertainty due to *limited physical tests* be induced or real?
    -   Induced. Because these physical tests are limited, the corresponding information is therefore limited, which creates induced uncertainty.

## Quantifying sampling uncertainty

<!-- --------------------------------------- -->

Using `plnorm()` gets rid of Monte Carlo error, but we still have uncertainty due to limited physical testing. Often we can use a CLT approximation to construct a confidence interval. However, with `plnorm()` we can't use a CLT approximation because it does not follow the assumptions of the central limit theorem (it's not a sum of iid random variables). Instead, we can use the *bootstrap* to approximate a confidence interval via resampling.

### **q6** The code below estimates a bootstrap CI on your POF estimate. Answer the questions under *observations* below.

```{r q6-task}
## NOTE: No need to edit; run and inspect
tidycustom <- function(est) {tibble(term = "pof", estimate = est)}

df_samples %>%
  bootstraps(times = 1000) %>%
  mutate(
    estimates = map(
      splits,
      ~ analysis(.x) %>% estimate_pof() %>% tidycustom()
    )
  ) %>%
  int_pctl(estimates)
```

**Observations**:

-   Does the confidence interval above account for uncertainty arising from *Monte Carlo approximation* of the POF? Why or why not?
    -   Yes, because it is a confidence interval, which is a range that accounts for uncertainty.
-   Does the confidence interval above account for uncertainty arising from *limited physical tests* (`df_samples`)? Why or why not?
    -   Yes. This confidence interval was generated using the bootstrap method, which is a method specifically chosen to deal with the uncertainty of limited datasets. Because of the use of the bootstrap method on the sample, the confidence interval does account for uncertainty from the sample's limited physical tests.
    -   No, because it still uses the same unchanged dataset as before, which has uncertainty both from the actual aluminum parts and the measurement of those parts.
-   Can you confidently conclude that `POF < 0.03`? Why or why not?
    -   No, because 0.03 is included in the estimation range of 0.001395635 to 0.047265. The true value of the POF can fall anywhere within this range, so I cannot be confident in saying that the POF is less than 0.03.
